{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices: Advanced Regression Techniques - Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook XGBRegressor(), ExtraTreesRegressor(), RandomForestRegressor(), GradientBoostingRegressor(), DecisionTreeRegressor(), AdaBoostRegressor() and LGBMRegressor() will be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. features: 79\n",
      "No. numerical features: 33\n",
      "No. ordinal features: 21\n",
      "No. (possible) categorical features: 25 \n",
      "\n",
      "num_cols: ['LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'LotFrontage'] \n",
      "\n",
      "ord_cols: ['OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond', 'Utilities', 'Functional', 'GarageFinish', 'PavedDrive', 'Alley', 'Fence', 'FireplaceQu', 'PoolQC'] \n",
      "\n",
      "cat_cols: ['MSSubClass', 'MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'CentralAir', 'Electrical', 'GarageType', 'SaleType', 'SaleCondition', 'MiscFeature'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from load_modules_files_functions_clean import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions introduced in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cv_val_score(my_s, print_best_est = True): # This function does not work properly when it is imported from load_modules_files_functions_clean; X[cols] is not updated correctly.   \n",
    "    best_est = my_s.best_estimator_\n",
    "    best_est.fit(X_train, y_train)\n",
    "    y_pred = best_est.predict(X_val)\n",
    "    val_score = rmsle(y_val, y_pred)\n",
    "    best_CV_score = my_s.best_score_        \n",
    "    print('Best CV score:', round(-best_CV_score, 5))\n",
    "    print('Validation score:', round(val_score, 5))\n",
    "    if print_best_est:\n",
    "        print(best_est)\n",
    "        \n",
    "def get_sub_csv(my_s, cols, name_csv): # There is a similar problem for this function as well.\n",
    "    print(name_csv)\n",
    "    best_est = my_s.best_estimator_    \n",
    "    best_est.fit(X[cols], y)\n",
    "    X_test = test[cols]\n",
    "    y_pred = best_est.predict(X_test)    \n",
    "    test_submission = pd.DataFrame({'Id':test['Id'], 'SalePrice':y_pred})\n",
    "    test_submission.to_csv(name_csv, index=False)\n",
    "\n",
    "def load_run_save_GSCV(key, param_grid, save_s = True):\n",
    "    global results\n",
    "    filename = key + '.joblib'\n",
    "    if os.path.isfile(filename):\n",
    "        my_s = joblib.load(filename)\n",
    "    else:  \n",
    "        my_s = GridSearchCV(ttr, param_grid = param_grid, cv = 5, scoring = rmsle_scorer, n_jobs = -1, verbose = 10, error_score = 'raise')\n",
    "        my_s = my_s.fit(X_train, y_train)\n",
    "        if save_s:\n",
    "            joblib.dump(my_s, filename)\n",
    "    best_est = my_s.best_estimator_\n",
    "    best_est.fit(X_train, y_train)   \n",
    "    y_pred = best_est.predict(X_val)\n",
    "    val_score = rmsle(y_val, y_pred)\n",
    "    best_CV_score = my_s.best_score_    \n",
    "    results_model = pd.Series({'Best CV score': -best_CV_score, 'Val score':val_score})\n",
    "    results_model.name = key\n",
    "    results = results.append(results_model)\n",
    "    return my_s\n",
    "\n",
    "def min_imp_filter(cols, feat_imps, min_imp):\n",
    "    feats_keep = list(feat_imps[feat_imps > min_imp].index)\n",
    "    cols_keep = []\n",
    "    for col in cols:\n",
    "        if col in feats_keep:\n",
    "            cols_keep.append(col)\n",
    "    return cols_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dataframe where the results are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'Best CV score':[], 'Val score':[]}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the general pipeline for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = ColumnTransformer([\n",
    "    ('imputer_num_cols', 'passthrough', slice(0, len(num_cols))),\n",
    "    ('imputer_ord_cols', 'passthrough', slice(len(num_cols), len(num_cols + ord_cols))),\n",
    "    ('imputer_cat_cols', 'passthrough' , slice(len(num_cols + ord_cols), len(num_cols + ord_cols + cat_cols))) \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "            ('scaler_num_cols', 'passthrough', slice(0, len(num_cols))),\n",
    "            ('scaler_ord_cols', 'passthrough', slice(len(num_cols), len(num_cols + ord_cols)))\n",
    "            #('category_encoder_cat_cols', None, slice(len(num_cols + ord_cols), len(num_cols + ord_cols + cat_cols)))\n",
    "], remainder = 'drop') # Temporary solution so that the model can be built without categorical features.\n",
    "\n",
    "steps = [\n",
    "    ('imputer', imputer),\n",
    "    ('preprocessor', preprocessor),         \n",
    "    ('model', None)\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "ttr = TransformedTargetRegressor(regressor = pipeline, func = np.log1p, inverse_func = np.expm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the default XGBRegressor() and define a special param_grid where no imputation is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr = XGBRegressor(objective = 'reg:squarederror', random_state = 1)\n",
    "key = 'xgbr_default'\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "    'regressor__imputer__imputer_num_cols': ['passthrough'],\n",
    "    'regressor__imputer__imputer_ord_cols': ['passthrough'],\n",
    "    'regressor__imputer__imputer_cat_cols': ['passthrough'],\n",
    "    'regressor__model': [xgbr],       \n",
    "    },    \n",
    "    {\n",
    "    'regressor__imputer__imputer_num_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_num_cols__strategy': ['mean', 'median', 'most_frequent', 'constant'],\n",
    "    'regressor__imputer__imputer_ord_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_ord_cols__strategy': ['mean', 'median', 'most_frequent', 'constant'],\n",
    "    'regressor__imputer__imputer_cat_cols': [SimpleImputer(strategy = 'most_frequent')],\n",
    "    'regressor__model': [xgbr],    \n",
    "    }\n",
    "]\n",
    "\n",
    "#my_s = load_run_save_GSCV(key, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the param_grid used for all other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'regressor__imputer__imputer_num_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_num_cols__strategy': ['mean', 'median', 'most_frequent', 'constant'],\n",
    "    'regressor__imputer__imputer_ord_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_ord_cols__strategy': ['mean', 'median', 'most_frequent', 'constant'],\n",
    "    'regressor__imputer__imputer_cat_cols': [SimpleImputer(strategy = 'most_frequent')],\n",
    "    'regressor__model': [None],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the default ExtraTreesRegressor()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etr = ExtraTreesRegressor(random_state = 1)\n",
    "key = 'etr_default'\n",
    "param_grid['regressor__model'] = [etr]\n",
    "#my_s = load_run_save_GSCV(key, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the default RandomForestRegressor()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state = 1)\n",
    "key = 'rfr_default'\n",
    "param_grid['regressor__model'] = [rfr]\n",
    "#my_s = load_run_save_GSCV(key, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the default GradientBoostingRegressor()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(random_state = 1)\n",
    "key = 'gbr_default'\n",
    "param_grid['regressor__model'] = [gbr]\n",
    "my_s = load_run_save_GSCV(key, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the default DecisionTreeRegressor()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor(random_state = 1)\n",
    "key = 'dtr_default'\n",
    "param_grid['regressor__model'] = [dtr]\n",
    "#my_s = load_run_save_GSCV(key, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the default AdaBoostRegressor()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abr = AdaBoostRegressor(random_state = 1)\n",
    "key = 'abr_default'\n",
    "param_grid['regressor__model'] = [abr]\n",
    "#my_s = load_run_save_GSCV(key, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the default LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmr = LGBMRegressor(random_state = 1)\n",
    "key = 'lgbmr_default'\n",
    "param_grid['regressor__model'] = [lgbmr]\n",
    "my_s = load_run_save_GSCV(key, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GradientBoostingRegressor() has both the best CV and val score. \n",
    "Let's submit this model and explore it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'gbr_default'\n",
    "my_s = load_run_save_GSCV(key, param_grid)\n",
    "print_cv_val_score(my_s, print_best_est = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'gbr_sub_num_ord.csv'\n",
    "#get_sub_csv(my_s, num_cols + ord_cols + cat_cols, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_s.best_estimator_.regressor_.named_steps.model\n",
    "feat_imps = pd.Series(model.feature_importances_, index=X[num_cols + ord_cols].columns)\n",
    "plt.figure(figsize = (17.5, 5))\n",
    "feat_imps.sort_values(ascending = False).plot(kind='bar', rot = 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a minimum importance filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_imp = 0\n",
    "#min_imp = 0.01\n",
    "min_imp = 0.001 # The best one found so far.\n",
    "#min_imp = 0.0001\n",
    "#min_imp = 10**(-5)\n",
    "\n",
    "min_imp = -1 # Keep all features.\n",
    "\n",
    "print('No. features before min importance filter:', len(num_cols + ord_cols))\n",
    "num_cols = min_imp_filter(num_cols, feat_imps, min_imp)\n",
    "ord_cols = min_imp_filter(ord_cols, feat_imps, min_imp)\n",
    "print('No. features after min importance filter', len(num_cols + ord_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must get new train and validation sets since the order in num_cols and ord_cols has changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = get_train_val_sets(X, y, num_cols + ord_cols + cat_cols) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild the general pipeline with the new num_cols and ord_cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = ColumnTransformer([\n",
    "    ('imputer_num_cols', 'passthrough', slice(0, len(num_cols))),\n",
    "    ('imputer_ord_cols', 'passthrough', slice(len(num_cols), len(num_cols + ord_cols))),\n",
    "    ('imputer_cat_cols', 'passthrough' , slice(len(num_cols + ord_cols), len(num_cols + ord_cols + cat_cols))) \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('scaler_num_cols', 'passthrough', slice(0, len(num_cols))),\n",
    "    ('scaler_ord_cols', 'passthrough', slice(len(num_cols), len(num_cols + ord_cols)))\n",
    "    #('category_encoder_cat_cols', None, slice(len(num_cols + ord_cols), len(num_cols + ord_cols + cat_cols)))\n",
    "], remainder = 'drop') # Temporary solution so that the model can be built without categorical features.\n",
    "\n",
    "steps = [\n",
    "    ('imputer', imputer),\n",
    "    ('preprocessor', preprocessor),         \n",
    "    ('model', None)\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "ttr = TransformedTargetRegressor(regressor = pipeline, func = np.log1p, inverse_func = np.expm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the same model with fewer features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(random_state = 1)\n",
    "key = 'gbr_default_min_imp' + str(min_imp)\n",
    "param_grid['regressor__model'] = [gbr]\n",
    "my_s = load_run_save_GSCV(key, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cv_val_score(my_s, print_best_est = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit if the model gets a better CV or Val score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'gbr_sub_num_ord_min_imp' + str(min_imp) + '.csv'\n",
    "#get_sub_csv(my_s, num_cols + ord_cols + cat_cols, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the feature importances again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_s.best_estimator_.regressor_.named_steps.model\n",
    "feat_imps = pd.Series(model.feature_importances_, index=X[num_cols + ord_cols].columns)\n",
    "plt.figure(figsize = (17.5, 5))\n",
    "feat_imps.sort_values(ascending = False).plot(kind='bar', rot = 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The order of the importances of different features has changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider potential categorical features after EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_cols = ['MSSubClass']\n",
    "#cat_cols = ['LandContour']\n",
    "#cat_cols = ['MSSubClass', 'LandContour']\n",
    "#cat_cols = ['MSSubClass', 'LandContour', 'MSZoning']\n",
    "#cat_cols = ['LandContour', 'MSZoning']\n",
    "#cat_cols = ['LandContour', 'MSZoning', 'LotShape', 'LotConfig']\n",
    "#cat_cols = ['MSSubClass', 'LandContour', 'MSZoning', 'LotShape', 'LotConfig']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** This is a quite inefficient way of working (adding each categorical feature one at a time).\n",
    "We will instead add all possible categorical features, perform OHE and see what the results are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must get new train and validation sets that take into account cat_cols. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = get_train_val_sets(X, y, num_cols + ord_cols + cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build pipeline with OneHotEndcoder() for the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = ColumnTransformer([\n",
    "    ('imputer_num_cols', 'passthrough', slice(0, len(num_cols))),\n",
    "    ('imputer_ord_cols', 'passthrough', slice(len(num_cols), len(num_cols + ord_cols))),\n",
    "    ('imputer_cat_cols', 'passthrough' , slice(len(num_cols + ord_cols), len(num_cols + ord_cols + cat_cols))) \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "            ('scaler_num_cols', 'passthrough', slice(0, len(num_cols))),\n",
    "            ('scaler_ord_cols', 'passthrough', slice(len(num_cols), len(num_cols + ord_cols))),\n",
    "            ('category_encoder_cat_cols', OneHotEncoder(handle_unknown = 'ignore'), slice(len(num_cols + ord_cols), len(num_cols + ord_cols + cat_cols)))\n",
    "]) \n",
    "\n",
    "steps = [\n",
    "    ('imputer', imputer),\n",
    "    ('preprocessor', preprocessor),         \n",
    "    ('model', None)\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "ttr = TransformedTargetRegressor(regressor = pipeline, func = np.log1p, inverse_func = np.expm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a param_grid where different imputation strategies for the categorical features are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'regressor__imputer__imputer_num_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_num_cols__strategy': ['mean', 'median', 'most_frequent', 'constant'],\n",
    "    'regressor__imputer__imputer_ord_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_ord_cols__strategy': ['mean', 'median', 'most_frequent', 'constant'],\n",
    "    'regressor__imputer__imputer_cat_cols': [SimpleImputer(fill_value = 'MISS')],\n",
    "    'regressor__imputer__imputer_cat_cols__strategy': ['most_frequent', 'constant'],\n",
    "    'regressor__model': [None],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(random_state = 1)\n",
    "key = 'gbr_default_min_imp' + str(min_imp) + '_cat'\n",
    "param_grid['regressor__model'] = [gbr]\n",
    "my_s = load_run_save_GSCV(key, param_grid, save_s = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cv_val_score(my_s, print_best_est = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the OHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = my_s.best_estimator_.regressor_.named_steps.preprocessor.named_transformers_.category_encoder_cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot feature importances (including OHE features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_s.best_estimator_.regressor_.named_steps.model\n",
    "feat_imps = pd.Series(model.feature_importances_, index = list(X[num_cols + ord_cols].columns) + list(ohe.get_feature_names(cat_cols)))\n",
    "plt.figure(figsize = (105, 10))\n",
    "feat_imps.sort_values(ascending = False).plot(kind='bar', rot = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. features after OHE:', len(model.feature_importances_))\n",
    "print('No. features after OHE larger than 0:', len(model.feature_importances_[model.feature_importances_ > 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Feature selection could (and probably should) be performed.\n",
    "It is worth noting that some levels of the OHE features are more important than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'gbr_sub_num_ord_cat_min_imp' + str(min_imp) + '.csv'\n",
    "#get_sub_csv(my_s, num_cols + ord_cols + cat_cols, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the best imputation strategies found in the previous param_grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_imputer = my_s.best_estimator_.regressor_.named_steps.imputer.named_transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the param_grid used for HPO of GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'regressor__imputer__imputer_num_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_num_cols__strategy': [my_imputer.imputer_num_cols.strategy],\n",
    "    'regressor__imputer__imputer_ord_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_ord_cols__strategy': [my_imputer.imputer_ord_cols.strategy],\n",
    "    'regressor__imputer__imputer_cat_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_cat_cols__strategy': [my_imputer.imputer_cat_cols.strategy],\n",
    "    'regressor__model': [None],    \n",
    "    'regressor__model__loss': ['ls', 'lad', 'huber', 'quantile'], \n",
    "    'regressor__model__learning_rate': [0.01, 0.02, 0.05, 0.1, 1],\n",
    "    'regressor__model__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'regressor__model__max_depth': [1, 2, 3, 4, 5, 6],\n",
    "    'regressor__model__max_features': [None, 'sqrt'],   \n",
    "    'regressor__model__min_samples_leaf': [1, 3, 5],\n",
    "    'regressor__model__min_samples_split': [2, 4, 8],\n",
    "    'regressor__model__ccp_alpha': [0, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# param_grid obtained after HPO on personal computer.\n",
    "param_grid = {\n",
    "    'regressor__imputer__imputer_num_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_num_cols__strategy': [my_imputer.imputer_num_cols.strategy],\n",
    "    'regressor__imputer__imputer_ord_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_ord_cols__strategy': [my_imputer.imputer_ord_cols.strategy],\n",
    "    'regressor__imputer__imputer_cat_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_cat_cols__strategy': [my_imputer.imputer_cat_cols.strategy],\n",
    "    'regressor__model': [None],    \n",
    "    'regressor__model__loss': ['huber'],\n",
    "    'regressor__model__learning_rate': [0.05],\n",
    "    'regressor__model__n_estimators': [500],\n",
    "    'regressor__model__max_depth': [3],\n",
    "    'regressor__model__max_features': ['sqrt'],\n",
    "    'regressor__model__min_samples_leaf': [1],\n",
    "    'regressor__model__min_samples_split': [8],\n",
    "    'regressor__model__ccp_alpha': [0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(random_state = 1)\n",
    "key = 'gbr_default_min_imp' + str(min_imp) + '_cat_HPO'\n",
    "param_grid['regressor__model'] = [gbr]\n",
    "my_s = load_run_save_GSCV(key, param_grid, save_s = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cv_val_score(my_s, print_best_est = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'gbr_sub_num_ord_cat_min_imp' + str(min_imp) + '_HPO_GBR.csv'\n",
    "get_sub_csv(my_s, num_cols + ord_cols + cat_cols, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot feature importances of the model where HPO of the GBR has been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = my_s.best_estimator_.regressor_.named_steps.preprocessor.named_transformers_.category_encoder_cat_cols\n",
    "model = my_s.best_estimator_.regressor_.named_steps.model\n",
    "feat_imps = pd.Series(model.feature_importances_, index = list(X[num_cols + ord_cols].columns) + list(ohe.get_feature_names(cat_cols)))\n",
    "plt.figure(figsize = (105, 10))\n",
    "feat_imps.sort_values(ascending = False).plot(kind='bar', rot = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. features after OHE:', len(model.feature_importances_))\n",
    "print('No. features after OHE larger than 0:', len(model.feature_importances_[model.feature_importances_ > 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** After the HPO we can see that two most important features remain the same, but that their relative importance has decreased; the model is now much better at using other features. \n",
    "It is also important to notice that the no. feature with importance > 0 has increased massively; from 112 out of 238 features, to 207 out of 240. \n",
    "It is, however, strange that len(feature_importances) = 240  now compared to 238 previously...\n",
    "The order of the importances is also quite different now after the two most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out some feature engineering ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['DiffYearRemodAddBuilt'] = X['YearRemodAdd'] - X['YearBuilt']\n",
    "X['DivGrLivAreaFullBath'] = X['GrLivArea'] / X['FullBath']\n",
    "X['DivGrLivAreaBedroomAbvGr'] = X['GrLivArea'] / X['BedroomAbvGr']\n",
    "\n",
    "#num_cols.append('DiffYearRemodAddBuilt')\n",
    "\n",
    "#num_cols.remove('YearRemodAdd')\n",
    "\n",
    "#num_cols.remove('YearBuilt')\n",
    "\n",
    "#num_cols.remove('YearRemodAdd')\n",
    "#num_cols.remove('YearBuilt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_cols = ['YearBuilt', 'YearRemodAdd', 'DiffYearRemodAddBuilt']\n",
    "plot_cols = ['GrLivArea', 'FullBath', 'BedroomAbvGr', 'DivGrLivAreaFullBath', 'DivGrLivAreaBedroomAbvGr']\n",
    "fig, axes = plt.subplots(1, len(plot_cols), figsize = (50, 10))\n",
    "for i, col in enumerate(plot_cols):\n",
    "    #sns.scatterplot(data = pd.concat([X[plot_cols], y], axis = 1), x = col, y = 'SalePrice', ax = axes.flat[i], alpha = 0.2)\n",
    "    sns.regplot(data = pd.concat([X[plot_cols], y], axis = 1), x = col, y = 'SalePrice', ax = axes.flat[i],  line_kws = {\"color\": \"red\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must get new train and validation sets that take into account the updated cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = get_train_val_sets(X, y, num_cols + ord_cols + cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = ColumnTransformer([\n",
    "    ('imputer_num_cols', 'passthrough', slice(0, len(num_cols))),\n",
    "    ('imputer_ord_cols', 'passthrough', slice(len(num_cols), len(num_cols + ord_cols))),\n",
    "    ('imputer_cat_cols', 'passthrough' , slice(len(num_cols + ord_cols), len(num_cols + ord_cols + cat_cols))) \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "            ('scaler_num_cols', 'passthrough', slice(0, len(num_cols))),\n",
    "            ('scaler_ord_cols', 'passthrough', slice(len(num_cols), len(num_cols + ord_cols))),\n",
    "            ('category_encoder_cat_cols', OneHotEncoder(handle_unknown = 'ignore'), slice(len(num_cols + ord_cols), len(num_cols + ord_cols + cat_cols)))\n",
    "]) \n",
    "\n",
    "steps = [\n",
    "    ('imputer', imputer),\n",
    "    ('preprocessor', preprocessor),         \n",
    "    ('model', None)\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "ttr = TransformedTargetRegressor(regressor = pipeline, func = np.log1p, inverse_func = np.expm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid obtained after HPO on personal computer.\n",
    "param_grid = {\n",
    "    'regressor__imputer__imputer_num_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_num_cols__strategy': [my_imputer.imputer_num_cols.strategy],\n",
    "    'regressor__imputer__imputer_ord_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_ord_cols__strategy': [my_imputer.imputer_ord_cols.strategy],\n",
    "    'regressor__imputer__imputer_cat_cols': [SimpleImputer(fill_value = -999)],\n",
    "    'regressor__imputer__imputer_cat_cols__strategy': [my_imputer.imputer_cat_cols.strategy],\n",
    "    'regressor__model': [None],    \n",
    "    'regressor__model__loss': ['huber'],\n",
    "    'regressor__model__learning_rate': [0.05],\n",
    "    'regressor__model__n_estimators': [500],\n",
    "    'regressor__model__max_depth': [3],\n",
    "    'regressor__model__max_features': ['sqrt'],\n",
    "    'regressor__model__min_samples_leaf': [1],\n",
    "    'regressor__model__min_samples_split': [8],\n",
    "    'regressor__model__ccp_alpha': [0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(random_state = 1)\n",
    "key = 'gbr_default_min_imp' + str(min_imp) + '_cat_HPO_FE'\n",
    "param_grid['regressor__model'] = [gbr]\n",
    "my_s = load_run_save_GSCV(key, param_grid, save_s = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cv_val_score(my_s, print_best_est = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log\n",
    "\n",
    "Default GradientBoostingRegressor() (54 features) ---> CV: 0.13070, Val: 0.13013, Test: 0.13677 (worse than the best XGBRegressor() but still really good)\n",
    "**Note:** We will use this model as the source of feature importances that will be used for our filter.\n",
    "\n",
    "min_imp = 0 (51 features) ---> CV: 0.13122, Val: 0.13039 (slightly worse)\n",
    "\n",
    "min_imp = 0.01 (14 features) ---> CV: 0.16076, Val: 0.17116 (much worse)\n",
    "\n",
    "min_imp = 0.001 (28 features) ---> CV: 0.15107, Val: 0.15470 (worse)\n",
    "\n",
    "min_imp = 0.0001 (45 features) ---> CV: 0.13063, Val: 0.12999, Test: 0.13692 (better CV and Val score, but slightly worse Test score)\n",
    "\n",
    "min_imp = 10^(-5) (50 features) ---> CV: 0.13110, Val: 0.13530 (worse)\n",
    "\n",
    "**UPDATE 2020-05-27**\n",
    "Found a major error in the code wrt. how num_cols and ord_cols are used to extract data from X in the pipeline. Must redo all the min_imp results\n",
    "\n",
    "min_imp = 0 (51 features) ---> CV: 0.13073, Val: 0.13092, Test: 0.13637 (better)\n",
    "\n",
    "min_imp = 0.01 (14 features) ---> CV: 0.13305, Val: 0.13488 (worse)\n",
    "\n",
    "min_imp = 0.001 (29 features) ---> CV: 0.13075, Val: 0.13140, Test: 0.13526 (better)\n",
    "\n",
    "min_imp = 0.0001 (45 features) ---> CV: 0.13092, Val: 0.13089, Test: 0.13692 (worse)\n",
    "\n",
    "min_imp = 10^(-5) (50 features) ---> CV: 0.13090, Val: 0.13425 (worse)\n",
    "\n",
    "**Note:** Could try 0.0001 < min_imp < 0.001\n",
    "\n",
    "**We will keep min_imp = 0.001.**\n",
    "\n",
    "**We will now consider potential categorical features.**\n",
    "\n",
    "MSSubClass (only) (15 levels) ---> CV: 0.13035, Val: 0.13169\n",
    "\n",
    "LandContour (only) (4 levels) ---> CV: 0.13042, Val: 0.13171\n",
    "\n",
    "MSSubClass + LandContour ---> CV: 0.13123, Val: 0.13058, Test: 0.13424 (better, even than xgboost)\n",
    "\n",
    "MSSubClass + LandContour + MSZoning ---> CV: 0.13058, Val: 0.12901, Test: 0.13831 (worse)\n",
    "\n",
    "LandContour + MSZoning ---> CV: 0.12852, Val: 0.12516, Test: 0.13730 (worse)\n",
    "**Note:** It seems as if MSSubClass does contain useful information.\n",
    "\n",
    "LandContour + MSZoning + LotShape---> CV: 0.12973, Val: 0.12885, Test: 0.13507 (worse)\n",
    "\n",
    "LandContour + MSZoning + LotShape + LotConfig---> CV: 0.12954, Val: 0.12718\n",
    "\n",
    "MSSubClass + LandContour + MSZoning + LotShape + LotConfig---> CV: 0.12988, Val: 0.12566, Test: 0.13651\n",
    "\n",
    "All cat_cols ---> CV: 0.12848, Val: 0.12391, Test: 0.13358 (better)\n",
    "\n",
    "All num_cols + ord_cols + cat_cols (min_imp = -1) ---> CV: 0.12817, Val: 0.12800, Test: 0.13248 (better)\n",
    "\n",
    "**HPO of the GradientBoostingRegressor()**\n",
    "\n",
    "Keep the previously used param_grid, where we try different imputations techniques, and add hyperparameters of the GradientBoostingRegressor() ---> GridSearchCV() will take too much time to finish (45 days).\n",
    "\n",
    "Extract the best hyperparameters found in the previous param_grid, and add hyperparameters  of the GradientBoostingRegressor() ---> CV: 0.12016, Val: 0.11978, Test: 0.12588 (better)\n",
    "**Note:** \n",
    "* GridSearchCV() would take too much time to finish on Kaggle (29h), but it was possible to run on my personal computer.\n",
    "* n_estimators = 500 was selected, which was the max. We should consider larger values.\n",
    "* max_features = 'sqrt' was selected, probably due to the high dimensional input matrix.\n",
    "* ccp_alpha = 0 was selected, which means that no regularization was applied. Might consider adding regularization manually and see if the Val score is improved.\n",
    "\n",
    "**Feature Engineering** (after HPO of the GradientBoostingRegressor())\n",
    "\n",
    "Add DiffYearRemodAddBuilt ---> CV: 0.12074, Val: 0.12628 (worse)\n",
    "\n",
    "Add DiffYearRemodAddBuilt, remove YearRemodAdd ---> CV: 0.12268, Val: 0.12000\n",
    "\n",
    "Add DiffYearRemodAddBuilt, remove YearBuilt ---> CV: 0.12198, Val: 0.12288\n",
    "\n",
    "Add DiffYearRemodAddBuilt, remove YearRemodAdd and YearBuilt ---> CV: 0.12437, Val: 0.12753\n",
    "\n",
    "**Comment:** It probably does not make sense to do FE after the HPO since we have such a highly optimized model. Better to work with one of the previous models that are fairly \"raw\". ---> Create new notebook for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various notes\n",
    "\n",
    "For the XGBRegressor(), when HPO of the pipeline was performed, the best model found did indeed perform better when some kind of missing value imputation is performed.\n",
    "\n",
    "The models, XGBRegressor() and GradientBoostingRegressor() in particular, seem to be able to handle many features as input and select the important ones (they can find the signal).\n",
    "\n",
    "The top 5 features chosen by GradientBoostingRegressor() are: OverallQual, GrLivArea, TotalBsmtSF, KitchenQual and ExterQual.\n",
    "As expected, we see clearly distinct ranges for each level, or a high correlation if the feature is continuous.\n",
    "\n",
    "The top 5 OHE features chosen by GradientBoostingRegressor() are: MSZoning_RM, CentralAir_N, MSZoning_RL, Neighborhood_Crawfor, CentralAir_Y.\n",
    "Not quite as obvious that these OHE features are valuable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
